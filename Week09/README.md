# 找到能够提升模型效果的特征组合

[代码过程](https://github.com/JustDoItGit/ML-000/blob/master/Week09/Lending%20Club.ipynb)

## lgb默认参数的表现

```text
{'binary_error-mean': [0.19985999999999998,
  0.19985999999999998,
  0.19985999999999998,
  0.19985999999999998,
  0.14916,
  0.12000999999999999,
  0.10207999999999999,
  0.09613999999999999,
  0.09107000000000001,
  0.08845],
 'binary_error-stdv': [1.99999999999978e-05,
  1.99999999999978e-05,
  1.99999999999978e-05,
  1.99999999999978e-05,
  0.0020941346661568845,
  0.0015242703172337918,
  0.0018993683160461554,
  0.0020594659501919437,
  0.002299695632034813,
  0.0020344532435030266]}
```

最优误差：0.08845

## 自动调参选择最优参数

最优参数：

```text
{'bagging_fraction': 0.9834654040956224,
 'boosting': 2,
 'device_type': 0,
 'drop_rate': 0.1997907495675753,
 'extra_tress': 1,
 'feature_fraction': 0.6838115928097803,
 'lambda_l1': 5.187010489820754,
 'lambda_l2': 5.837652960694227,
 'learning_rate': 0.07163325236711929,
 'metric': 0,
 'min_gain_to_split': 0.5254984036581041,
 'num_leaves': 0,
 'num_rounds': 0,
 'num_threads': 0,
 'objective': 0,
 'uniform_drop': 0}
```

## 特征选择

### 全量特征baseline

效果好于lgb默认参数

以此结果作为后续调参的基准线：0.08036000000000001

### 每次去除一个feature变量

删除单一变量结果表现前五的结果：

```text
('discrete_purpose_1_one_hot', 0.07976000000000001),
 ('discrete_purpose_5_one_hot', 0.07977999999999999),
 ('discrete_emp_length_8_one_hot', 0.07986),
 ('discrete_sub_grade_6_one_hot', 0.07987999999999999),
 ('discrete_grade_3_one_hot', 0.07988000000000002),
 ('discrete_addr_state_44_one_hot', 0.07992000000000002)
```

### 将大于baseline的结果叠加组合删除

```text
{2: 0.0803,
 3: 0.0806,
 4: 0.07962,
 5: 0.08028000000000002,
 6: 0.08018000000000003,
 7: 0.08008,
 8: 0.08042,
 9: 0.08032000000000002,
 10: 0.08027999999999999,
 11: 0.08079999999999998,
 12: 0.08115999999999998,
 13: 0.08080000000000001,
 14: 0.08035999999999999,
 15: 0.08066}
```

结果是前删除上一步骤前四组效果较好：0.07962

### 由于上面都是onehot之后的变量，尝试删除所有purpose变量看看效果

结果为：0.08026

删除一个变量的全部onehot效果不好

### 变量关于loan_status的相关性系数过滤

过滤到相关性低于0.01的特征

结果为：0.07984

好于基准

### 结论

对分类变量做onehot对结果有第一定的提升，鉴于拿到的数据本身就是onehot之后的，就不费力去还原one了。

<font color="red">特征选择提升有限，最好的提升值为：0.0007400000000000184，【去除四个特征：('discrete_purpose_1_one_hot',  'discrete_purpose_5_one_hot', 'discrete_emp_length_8_one_hot', 'discrete_sub_grade_6_one_hot')】</font>

<font color="red">应该选用更好的特征创造或特征提取来寻找高级特征。</font>

<font color="red">这要依赖对数据经EDA，鉴于时间有限，留待以后优化。</font>



